{
  "metadata": {
    "name": "Lesson2_HW_Bogdanova",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql import SparkSession"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nspark \u003d SparkSession \\\n    .builder \\\n    .appName(\"TestApp\") \\\n    .getOrCreate()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nurl \u003d \u0027https://s3.amazonaws.com/apache-zeppelin/tutorial/bank/bank.csv\u0027\nfrom pyspark import SparkFiles\nspark.sparkContext.addFile(url)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf \u003d spark.read.csv(\"file://\"+SparkFiles.get(\"bank.csv\"), sep\u003d\u0027;\u0027, header\u003dTrue, inferSchema\u003dTrue)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.registerTempTable(\"bank\")"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect age, count(*) as cnt\nfrom bank \ngroup by age\norder by cnt desc;"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect distinct age\nfrom bank\norder by age desc limit 1;"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect age, count(*) as cnt\nfrom bank \nwhere age\u003c\u003d${age\u003d87}\ngroup by age;"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect distinct marital from bank;"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect age, count(*) as cnt\nfrom bank \nwhere marital\u003d\u0027${marital\u003dsingle,divorced|married|single}\u0027\ngroup by age;"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.dtypes"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import StringType\ndf \u003d df.withColumn(\"age\",col(\"age\").cast(StringType()))\ndf.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Не поняла, что именно надо сделать в данном задании. Сделала буквально.\ntypes_dict \u003d {\"str\": \"StringType\", \"int\": \"IntegerType\", \"float\": \"FloatType\", \"list\": \"ArrayType\", \"dict\": \"MapType\", \"bool\": \"BooleanType\", \"bytes\": \"ByteType\", \"bytearray\": \"ByteType\"}\ndef python_type_to_pyspark(x):\n    return types_dict[x]\n    \npython_type_to_pyspark(\"str\")"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}